Tools used- Jupyter Notebook

clean text data like tockenization and stemming etc. Train the data and then get the nodes. Text preprocessing and then vectorize the text. Then start feeding model.
The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text. It provides a high-level api to flexibly implement a variety of cleaning methods. Preprocessing includes convert to lowercase, remove hyphen, remove numbers, remove words which consist of less than 2 characters, lemmatize. We will map each product into a real vector node, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model. We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the data in the test set that could help us better prepare the data are unavailable in the preparation of data used for training the model. Creating model we can use LogisticRegression, GaussianProcessClassifier, GradientBoostingClassifier, MLPClassifier, LGBMClassifier, SGDClassifier, Keras but SVC works better. Train model procedure includes train it with the whole train data for submission. After the model is developed, we will need to make predictions on new products. 